{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def padding_function(data: Tensor) -> Tensor:\n",
    "    return F.pad(data, (2, 2, 2, 2), mode='constant', value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "composed_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), # performs scaling for image datasets between range(0-1)\n",
    "    #padding_function,  # Pad from 28x28 to 32x32\n",
    "])\n",
    "\n",
    "train_val_set = MNIST(\"./data/train\", train=True, transform=composed_transforms, download=True)\n",
    "test_set = MNIST(\"./data/test\", train=False, transform=composed_transforms, download=True)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset=train_val_set, lengths=[.9, .1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note \n",
    "```\n",
    "If iteration performed over a dataset with transforms, transform operations cen be triggered, if not, will be triggered on train loop and may slow down the process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 2\n",
      "tensor([0.3843, 0.9176, 0.9176, 0.9176, 0.2510, 0.3451, 0.7608, 0.2980, 0.2980,\n",
      "        0.8235, 0.8353, 0.8353, 0.9490, 0.9922, 0.9922, 0.9922, 0.9294, 0.1098,\n",
      "        0.0667, 0.8078, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.1333, 0.2980, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.1333, 0.1216, 0.8510, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.7216, 0.0549, 0.3686, 0.8275, 0.6863,\n",
      "        0.4824, 0.4824, 0.6588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7412, 0.0275,\n",
      "        0.0157, 0.0118, 0.4235, 0.9216, 0.9922, 0.9922, 0.9922, 0.7451, 0.2902,\n",
      "        0.0196, 0.6275, 0.9922, 0.9922, 0.9922, 0.9922, 0.7059, 0.0471, 0.1843,\n",
      "        0.6000, 0.9922, 0.9922, 0.9922, 0.9922, 0.6902, 0.0667, 0.2627, 0.9059,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.8392, 0.2157, 0.0784, 0.1922, 0.1922,\n",
      "        0.2784, 0.3922, 0.1922, 0.0431, 0.5608, 0.9098, 0.9922, 0.9922, 0.9922,\n",
      "        0.9451, 0.6941, 0.0667, 0.0235, 0.4627, 0.6471, 0.6471, 0.7882, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.0157, 0.6863, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.2824, 0.2196, 0.5647, 0.6471, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.4627, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.6902, 0.5059, 0.9725,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.7961, 0.5882, 0.4157, 0.0510, 0.0510, 0.0196,\n",
      "        0.9961, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9294, 0.3216, 0.1333, 0.0667, 0.5725, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9059, 0.7490, 0.3176, 0.1373, 0.4980,\n",
      "        0.9686, 0.9922, 0.8824, 0.8314, 0.5529, 0.2941, 0.1922, 0.3176, 0.3765,\n",
      "        0.1216])\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+us074ZeM9Wt4Liy0C5lgnhE0UpKqrIehyxAz7dcc9KteL/hZ4i8E6Pbanqv2NreeQREQSlmjcjIDAgeh6ZHH0riaK+pvhnd+NR4etb3xRd2ul6FY24WMSxBZpkUYBkZj8gAHoCf1rz/4k+I/E3xMuk0/QNA1KTQ7Z/MjdLZj9oPQSZxwOTjnoefbyPUtNvdH1CWw1G1ltbuIgSQyrtZcgEZHuCD+Na/gXVNL0TxtpWpazbmewt5S8iBd2DtO1sd9rbWx7V9FeIfjP8PUhWOVjrJRhIkUdrvVWA4OXAAPJ5HIryzWf2gPFV3I0ekR2el2qnEQSESOFB4B3ZXOOOAK801bVr/XdTm1LU7l7m8nIMkr4y2AAOnsAPwqlRRRX//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4AWNgGOwg6dWrVw4Yjow45ONz6erVT3///n0rgyYreA4oCgNKEEkmmBprfRgLQcMlL99BCO5+A2EzwoXiSu+cYdANBfIPeP2Ei8IYLMwMOa+Btu7zgokg06wLQA7awoMsBmPrzwTJbRCE8ZFp5+cguWvmQDERQ0P3FEOEJHf/C6DUx42CbGVLl54EKfsLl2RZD+Z7WExYCmagSJbBhWCMDY0MDFB//mCFG8LAcO70lcUMP37DRf7BNPx9G6SE7t5JUMnn+wPhGuAM5frvfx9e6fU2hIuAGFA7GRgC+fc/RJEZjBwAnHeLU6iREToAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = iter(train_set).__next__()\n",
    "print(X.shape, y)\n",
    "print(X[X != 0]) # already scaled between 0-1\n",
    "transforms.ToPILImage()(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Input -> (batch_size,1,32,32)\n",
    "            # Layer 1\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=6,\n",
    "                kernel_size=5,\n",
    "                padding=2,\n",
    "                stride=1), # -> (batch_size,6,28,28)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), # -> (batch_size,6,14,14)\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(\n",
    "                in_channels=6,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=0), # -> (batch_size,16,10,10)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2), # (batch_size,16,5,5)\n",
    "        )\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Flatten(), # (batch_size,400)\n",
    "            nn.Linear(in_features=400, out_features=120), # (batch_size,401200)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84), # (batch_size,84)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=10), # (batch_size,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fully_connected(self.feature_extractor(x))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fully_connected): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LeNet5                                   [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 16, 5, 5]             --\n",
       "│    └─Conv2d: 2-1                       [1, 6, 28, 28]            156\n",
       "│    └─Tanh: 2-2                         [1, 6, 28, 28]            --\n",
       "│    └─AvgPool2d: 2-3                    [1, 6, 14, 14]            --\n",
       "│    └─Conv2d: 2-4                       [1, 16, 10, 10]           2,416\n",
       "│    └─Tanh: 2-5                         [1, 16, 10, 10]           --\n",
       "│    └─AvgPool2d: 2-6                    [1, 16, 5, 5]             --\n",
       "├─Sequential: 1-2                        [1, 10]                   --\n",
       "│    └─Flatten: 2-7                      [1, 400]                  --\n",
       "│    └─Linear: 2-8                       [1, 120]                  48,120\n",
       "│    └─Tanh: 2-9                         [1, 120]                  --\n",
       "│    └─Linear: 2-10                      [1, 84]                   10,164\n",
       "│    └─Tanh: 2-11                        [1, 84]                   --\n",
       "│    └─Linear: 2-12                      [1, 10]                   850\n",
       "==========================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 0.30\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "summary(model, input_size=(1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size=16\n",
    "\n",
    "optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
    "loss_fn =  CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x23a2a920e10>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x23a06ceb310>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x23a2a593390>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3375 [00:06<?, ?it/s]\n",
      "  0%|          | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\paper-implementations\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import sys\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        X_train, y_train = batch\n",
    "        print(X_train.shape)  \n",
    "        sys.exit()\n",
    "        # X_train = X_train.to(device)\n",
    "        # y_train = y_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn()        \n",
    "        \n",
    "    \"\"\"\n",
    "    for (X_val , y_val) in val_dataloader:\n",
    "        pass\n",
    "    \"\"\"  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
